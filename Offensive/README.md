# Offensive Language Detection

This folder contains datasets, notebooks, and results related to **Offensive Language Detection** within the **Context-Aware Toxicity Benchmarking** project. The objective is to benchmark various **offensive language classification models** using multiple datasets and evaluation metrics.

## ğŸ“Œ Folder Structure
```
ğŸ“‚ Offensive
â”‚â”€â”€ ğŸ“‚ notebooks      # Jupyter Notebooks for training & evaluation
â”‚â”€â”€ ğŸ“‚ dataset        # Datasets for offensive language classification
â”‚â”€â”€ ğŸ“‚ utils          # Utility function scripts 
â”‚â”€â”€ ğŸ“œ requirements.txt # Dependencies for running the models
```

## ğŸ“¥ Dataset Information
The datasets used in this folder **are not stored directly in this repository** due to size limitations. You can access them from the following sources:

- **OLID (Offensive Language Identification Dataset)** ([Hugging Face](https://huggingface.co/datasets/christophsonntag/OLID))
  - Contains 14,100 tweets labeled at three hierarchical levels: offensive/not offensive, type of offense, and target identification.
- **OffensiveLang** ([Hugging Face](https://huggingface.co/datasets/AmitDasRup123/OffensiveLang))
  - Community-based dataset of 8,270 texts generated by ChatGPT 3.5, covering 38 different target groups.
- **TweetEval** ([Hugging Face](https://huggingface.co/datasets/cardiffnlp/tweet_eval))
  - A benchmark for multiple Twitter-related classification tasks, including offensive speech.
- **TDavidson Dataset** ([Hugging Face](https://huggingface.co/datasets/tdavidson/hate_speech_offensive))
  - 24,802 tweets labeled as Hate, Offensive (but not hate), or Neither.

Make sure to **download the datasets manually** and place them in the `dataset/` folder before running any experiments.

## ğŸ—ï¸ Models Used
This benchmark evaluates multiple **Offensive Language Detection Models**, including:

1. **Arash-Rasouli/BERT** ([Hugging Face](https://huggingface.co/Arash-Rasouli/BERT))
   - Training Data: Unknown
   - Last Month Downloads: 198
2. **Silvgrad/DistilRoBERTa** ([Hugging Face](https://huggingface.co/Silvgrad/DistilRoBERTa))
   - Trained on OLID dataset
   - Accuracy: 0.86 | F1-score: 0.77
3. **Cardiff NLP RoBERTa (twitter-roberta-base-offensive)** ([Hugging Face](https://huggingface.co/cardiffnlp/twitter-roberta-base-offensive))
   - Trained on TweetEval-Offensive
   - Accuracy: 0.86 | F1-score: 0.78
4. **KoalaAI/OffensiveSpeechDetector** ([Hugging Face](https://huggingface.co/KoalaAI/OffensiveSpeechDetector))
   - Trained on TweetEval-Offensive
   - Accuracy: 0.747 | Macro F1: 0.709
5. **tasksource/deberta-small-long-nli** ([Hugging Face](https://huggingface.co/tasksource/deberta-small-long-nli))
   - Trained on multiple datasets, including TDavidson, for zero-shot classification.
6. **marcoorasch/llama-3.2-3B-instruct-hatespeech-offensive-classification** ([Hugging Face](https://huggingface.co/marcoorasch/llama-3.2-3B-instruct-hatespeech-offensive-classification))
   - Uses LLaMA-3.2-3B model for detecting hate speech and offensive content.

## ğŸš€ Running the Experiments
### 1ï¸âƒ£ Install Dependencies
Ensure that you have the required dependencies installed:
```bash
pip install -r requirements.txt
```

### 2ï¸âƒ£ Run Model Evaluation
Execute the Jupyter notebooks in the `notebooks/` folder to:
- Load datasets
- Fine-tune and evaluate models
- Compute accuracy, F1-score, and bias metrics

Example command:
```bash
jupyter notebook notebooks/offensive_speech_evaluation.ipynb
```

### 3ï¸âƒ£ View Results
Results, including model performance metrics, confusion matrices, and bias evaluations, are saved in the `results/` folder.

## ğŸ“Š Evaluation Metrics
Models are compared based on:
- **Accuracy & F1-score** across different datasets
- **Bias Metrics**
  - **Subgroup AUC** (Detects biases for specific communities)
  - **BPSN AUC** (False positives for neutral speech)
  - **BNSP AUC** (False negatives for offensive speech)
- **Generalized Mean of Bias (GMB) AUC**

## ğŸ“ References
- [OLID Dataset](https://huggingface.co/datasets/christophsonntag/OLID)
- [OffensiveLang Dataset](https://huggingface.co/datasets/AmitDasRup123/OffensiveLang)
- [TweetEval Dataset](https://huggingface.co/datasets/cardiffnlp/tweet_eval)
- [TDavidson Hate Speech Dataset](https://huggingface.co/datasets/tdavidson/hate_speech_offensive)
- [BERT for Offensive Language Detection](https://huggingface.co/Arash-Rasouli/BERT)
- [RoBERTa for TweetEval-Offensive](https://huggingface.co/cardiffnlp/twitter-roberta-base-offensive)
- [KoalaAI Offensive Speech Model](https://huggingface.co/KoalaAI/OffensiveSpeechDetector)
- [HateGuard - Multi-task Offensive Language Detection](https://github.com/CactiLab/HateGuard)

---
For any issues, please open an **issue** or contribute via **pull request**! ğŸš€
